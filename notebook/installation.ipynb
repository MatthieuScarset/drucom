{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1711de92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293d0ba4-7152-4fb5-84c4-af21adfc6f0c",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "As explained in the README file, you should have already fetched the datasets locally. \n",
    "\n",
    "See `../README.md` and `../script/fetch_drupal_data.sh`.\n",
    "\n",
    "<details>\n",
    "    <summary>Click to check data folder structure</summary>\n",
    "    \n",
    "```bash\n",
    "data\n",
    "├── csv\n",
    "│   └── countries.csv\n",
    "└── json\n",
    "    ├── pages_event\n",
    "    │   ├── page_0.json\n",
    "    │   └── page_x.json\n",
    "    ├── pages_organization\n",
    "    │   ├── page_0.json\n",
    "    │   └── page_x.json\n",
    "    ├── pages_user\n",
    "    │   ├── page_0.json\n",
    "    │   └── page_x.json\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2810b6",
   "metadata": {},
   "source": [
    "## Convertion\n",
    "\n",
    "There is too much data fetched from *d.o* to simply load it into Pandas.\n",
    "\n",
    "For instance as of today (March 23rd, 2025), there are approximately 42k pages for Users - see [this endpoint](https://www.drupal.org/api-d7/user.json?sort=uid&direction).\n",
    "\n",
    "Each page contains a list of 50 users summing to almost 2.1 million users.\n",
    "\n",
    "We must convert this data from JSON to a more efficient format for big data.\n",
    "\n",
    "We choose to use `parquet` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44818719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def merge_drupal_pages(input_folder: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge a list of dataframes into a single dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_folder : str\n",
    "        The path to the folder containing the JSON files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Merged dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to store dataframes\n",
    "    dataframes = []\n",
    "\n",
    "    # Dynamically set max_pages to the total number of JSON files in the directory\n",
    "    max_pages = len([f for f in os.listdir(\n",
    "        input_folder) if f.endswith(\".json\")])\n",
    "\n",
    "    # For testing purposes, limit the number of pages to process.\n",
    "    # max_pages = 1000\n",
    "\n",
    "    # Get a sorted list of JSON files\n",
    "    json_files = sorted(\n",
    "        [f for f in os.listdir(input_folder) if f.endswith(\".json\")],\n",
    "        # Sort by page number\n",
    "        key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0])\n",
    "    )[:max_pages]  # Limit to max_pages files\n",
    "\n",
    "    # Iterate over the selected JSON files\n",
    "    for filename in json_files:\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        # Read the JSON file into a dataframe\n",
    "        df = pd.read_json(file_path, lines=True)\n",
    "        # Append the dataframe to the list\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Concatenate all dataframes\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    print(\"Data concatenated and ready to be merged!\")\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd9d7eb",
   "metadata": {},
   "source": [
    "**⚠️ Warning**: the cell below can takes a long time to runs (last run took ~8.5 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e38f9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data concatenated and ready to be merged!\n",
      "Data successfully merged and saved to ../data/module_terms.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save the merged dataframe as a parquet file\n",
    "# Merge data pages into one DataFrame.\n",
    "# for name in ['organization', 'event', 'user', 'module', 'module_terms']:\n",
    "for name in ['module_terms']:\n",
    "    output_file = f\"../data/{name}.parquet\"\n",
    "    merged_df = merge_drupal_pages(f\"../data/json/pages_{name}\")\n",
    "    merged_df.to_parquet(output_file, index=False)\n",
    "    print(f\"Data successfully merged and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c98e9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the parquet file: ../data/user.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2093637, 18)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(f\"Reading the parquet file: {output_file}\")\n",
    "df = pd.read_parquet(output_file)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afe6345",
   "metadata": {},
   "source": [
    "## Fetching user activity log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b0e17e",
   "metadata": {},
   "source": [
    "### Comments \n",
    "\n",
    "Given that dozens or hundreds of pages of comments existing per user and we have \n",
    "more than two millions users, fetching all comments is too much data approximating 200 millions records.\n",
    "\n",
    "We can still get relevant information from_d.o_. \n",
    "\n",
    "We need to count comments per user and get the first and last comments' dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd43ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique UIDs: 2093637\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "df = pd.read_parquet('../data/user.parquet')\n",
    "uids = df['id'].unique()\n",
    "# Print UIDs to one JSON file.\n",
    "with open('../data/json/user_uids.json', 'w') as f:\n",
    "    json.dump(uids.tolist(), f)\n",
    "# Print the number of unique UIDs\n",
    "print(f\"Number of unique UIDs: {len(uids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869dfd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch comments for each user and add them to the dataframe\n",
    "df['comments_count'] = df['id'].apply(fetch_comments_by_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d5ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          (1, 2, 3)\n",
       "1          (1, 2, 3)\n",
       "2          (1, 2, 3)\n",
       "3          (1, 2, 3)\n",
       "4          (1, 2, 3)\n",
       "             ...    \n",
       "2093632    (1, 2, 3)\n",
       "2093633    (1, 2, 3)\n",
       "2093634    (1, 2, 3)\n",
       "2093635    (1, 2, 3)\n",
       "2093636    (1, 2, 3)\n",
       "Name: comments_count, Length: 2093637, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comments_count']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57569f6",
   "metadata": {},
   "source": [
    "## Next step\n",
    "\n",
    "🔎 You can now open [the exploration](./exploration.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drucom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
